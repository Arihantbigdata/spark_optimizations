{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arihant Shashank (Big data and aws engineer)\n",
    "## CGS_Task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem which is understood:\n",
    "This problem is basically to to get the streamed data which is in json format and join it with static hive table and create a new hive table\n",
    "\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "1) I have used all the data set creation through python only , not calling any file specifically to make sure everything is visible in one code\n",
    "\n",
    "\n",
    "2) Kafka setup i dont have , so kindly consider my kafka data as static data here \n",
    "\n",
    "\n",
    "3) Some optimization i will be using because the data of hive will be smaller as compared to kafka data so hive data needs to be broadcasted\n",
    "\n",
    "\n",
    "4) Kafka i have take 10 records and hive i have taken 12 records\n",
    "\n",
    "5) As i dont have any scheduler, so i will be shceduling it in aws glue or autosys based to event trigger of kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"cgs_task\").getOrCreate()\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the json table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets assume we have a data landing from the kafka into a specific s3 or hdfs location with below format\n",
    "# kakfa json \n",
    "cutomer_info=cutomer_info=[{\n",
    "  \"cust_id\": 101,\n",
    "  \"age\": 20,\n",
    "  \"gender\": \"male\",\n",
    "  \"City\": \"hyd\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 102,\n",
    "  \"age\": 21,\n",
    "  \"gender\": \"female\",\n",
    "  \"City\": \"blr\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 103,\n",
    "  \"age\": 22,\n",
    "  \"gender\": \"male\",\n",
    "  \"City\": \"kol\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 104,\n",
    "  \"age\": 22,\n",
    "  \"gender\": \"male\",\n",
    "  \"City\": \"ixr\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 105,\n",
    "  \"age\": 23,\n",
    "  \"gender\": \"male\",\n",
    "  \"City\": \"del\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 106,\n",
    "  \"age\": 25,\n",
    "  \"gender\": \"female\",\n",
    "  \"City\": \"mas\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 107,\n",
    "  \"age\": 26,\n",
    "  \"gender\": \"female\",\n",
    "  \"City\": \"mum\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 108,\n",
    "  \"age\": 29,\n",
    "  \"gender\": \"female\",\n",
    "  \"City\": \"pune\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 109,\n",
    "  \"age\": 30,\n",
    "  \"gender\": \"female\",\n",
    "  \"City\": \"dubai\"\n",
    "},\n",
    "{\n",
    "  \"cust_id\": 110,\n",
    "  \"age\": 704,\n",
    "  \"gender\": \"female\",\n",
    "  \"City\": \"texas\"\n",
    "}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the hive table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "schema= StructType([StructField(\"email_id\",StringType(), True), \\\n",
    "                    StructField(\"cust_id\",IntegerType(),True)\n",
    "                   ])\n",
    "\n",
    "hive_data =data2= [(\"xyz@gmail.com\",102),\n",
    "    (\"abc@gmail.com\",103),\n",
    "    (\"alpha@ymail.com\",104),\n",
    "    (\"asd@yahoo.com\",105),\n",
    "    (\"qwerty@outlook.com\",106)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data(cutomer_info):\n",
    "    \"\"\"this function will read the json data defined above by converting into rdd\n",
    "    cutomer_info is the json dataset\"\"\"\n",
    "    custi_info_rdd=sc.parallelize(cutomer_info)\n",
    "    customer_info_read=spark.read.json(custi_info_rdd)\n",
    "    return customer_info_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hive_data(hive_data,schema):\n",
    "    \"\"\"reading the data for hive hive_data= dataset which i created above,\n",
    "    schema is the schema i defined above for this schema\"\"\"\n",
    "    hive_table = spark.createDataFrame(data=hive_data,schema=schema)\n",
    "    return hive_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question :The ask is to write Spark code which will read both the sources in a recurring scheduled job(daily/weekly etc) and write out .csv \n",
    "As i dont have any scheduler, so i will be shceduling it in aws glue or autosys based to event trigger of kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_dataset(broadcast_hive_table,customer_info_read):\n",
    "    \"\"\"broadcast_hive_table: this will be hive table\n",
    "        customer_info_read: this is json df\n",
    "        this function will create the overall dataset\"\"\"\n",
    "    full_dataset_data=customer_info_read.join(broadcast_hive_table,customer_info_read.cust_id==broadcast_hive_table.cust_id,\"left\").\\\n",
    "                    select(\"email_id\",\n",
    "                          customer_info_read[\"cust_id\"],\n",
    "                          \"City\",\n",
    "                          \"age\",\n",
    "                          \"gender\")\n",
    "    return full_dataset_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-----+---+------+\n",
      "|          email_id|cust_id| City|age|gender|\n",
      "+------------------+-------+-----+---+------+\n",
      "|              null|    101|  hyd| 20|  male|\n",
      "|     xyz@gmail.com|    102|  blr| 21|female|\n",
      "|     abc@gmail.com|    103|  kol| 22|  male|\n",
      "|   alpha@ymail.com|    104|  ixr| 22|  male|\n",
      "|     asd@yahoo.com|    105|  del| 23|  male|\n",
      "|qwerty@outlook.com|    106|  mas| 25|female|\n",
      "|              null|    107|  mum| 26|female|\n",
      "|              null|    108| pune| 29|female|\n",
      "|              null|    109|dubai| 30|female|\n",
      "|              null|    110|texas|704|female|\n",
      "+------------------+-------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    hive_table = spark.createDataFrame(data=hive_data,schema=schema)\n",
    "    broadcast_hive_table=broadcast(hive_table)\n",
    "    # here i am brodcasting the hive table\n",
    "    customer_info_read= read_json_data(cutomer_info)\n",
    "    full_dataset=full_dataset(broadcast_hive_table,customer_info_read)\n",
    "    full_dataset.show()\n",
    "    full_dataset.toPandas().to_csv(\"csg_probelm_pyspark.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
