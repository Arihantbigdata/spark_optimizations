{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Function with code snippets (ab sab clear hai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"metadata approach\").getOrCreate()\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext\n",
    "sc= SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = spark.read.csv(\"C:\\\\Users\\\\ariha\\\\Desktop\\\\pyspark\\\\empsalary.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark sql approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a temporary view just to use spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_save= read_file.createTempView(\"emp_table\")\n",
    "reading_table = spark.sql(\"select * from emp_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+\n",
      "|EmpID|     Name|Department|Salary|\n",
      "+-----+---------+----------+------+\n",
      "|  101|  Arihant|        A1|  1000|\n",
      "|  102| Shashank|        A1|  1000|\n",
      "|  103|     Ranu|        A1|   500|\n",
      "|  104|  Sangram|        A1|  3000|\n",
      "|  105|    Singh|        A1|  1000|\n",
      "|  106|   prapti|        A2|    10|\n",
      "|  107|    kamal|        A2|    10|\n",
      "|  108|     Dulo|        A2|    12|\n",
      "|  109|   Kamali|        A2|    12|\n",
      "|  110|Prajapati|        A2|    13|\n",
      "|  111|   Prpait|        A2|    12|\n",
      "|  112|     Mumu|        A3|  1000|\n",
      "|  113|    Shivu|        A3|  1000|\n",
      "+-----+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reading_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_definition = spark.sql(\"\"\"select EmpID,\n",
    "                            Name,\n",
    "                            Department,\n",
    "                            Salary, \n",
    "                            RANK() OVER(PARTITION BY Department ORDER BY Salary) AS RANK_SALARY from emp_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+-----------+\n",
      "|EmpID|     Name|Department|Salary|RANK_SALARY|\n",
      "+-----+---------+----------+------+-----------+\n",
      "|  106|   prapti|        A2|    10|          1|\n",
      "|  107|    kamal|        A2|    10|          1|\n",
      "|  108|     Dulo|        A2|    12|          3|\n",
      "|  109|   Kamali|        A2|    12|          3|\n",
      "|  111|   Prpait|        A2|    12|          3|\n",
      "|  110|Prajapati|        A2|    13|          6|\n",
      "|  112|     Mumu|        A3|  1000|          1|\n",
      "|  113|    Shivu|        A3|  1000|          1|\n",
      "|  101|  Arihant|        A1|  1000|          1|\n",
      "|  102| Shashank|        A1|  1000|          1|\n",
      "|  105|    Singh|        A1|  1000|          1|\n",
      "|  104|  Sangram|        A1|  3000|          4|\n",
      "|  103|     Ranu|        A1|   500|          5|\n",
      "+-----+---------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_definition.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dense_rank definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_rank_def= spark.sql(\"\"\"select EmpID,\n",
    "                            Name,\n",
    "                            Department,\n",
    "                            Salary,\n",
    "                            DENSE_RANK() over(PARTITION BY Department order by Salary) as desnse_rank from emp_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+-----------+\n",
      "|EmpID|     Name|Department|Salary|desnse_rank|\n",
      "+-----+---------+----------+------+-----------+\n",
      "|  106|   prapti|        A2|    10|          1|\n",
      "|  107|    kamal|        A2|    10|          1|\n",
      "|  108|     Dulo|        A2|    12|          2|\n",
      "|  109|   Kamali|        A2|    12|          2|\n",
      "|  111|   Prpait|        A2|    12|          2|\n",
      "|  110|Prajapati|        A2|    13|          3|\n",
      "|  112|     Mumu|        A3|  1000|          1|\n",
      "|  113|    Shivu|        A3|  1000|          1|\n",
      "|  101|  Arihant|        A1|  1000|          1|\n",
      "|  102| Shashank|        A1|  1000|          1|\n",
      "|  105|    Singh|        A1|  1000|          1|\n",
      "|  104|  Sangram|        A1|  3000|          2|\n",
      "|  103|     Ranu|        A1|   500|          3|\n",
      "+-----+---------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dens_rank_def.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row_number defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_number= spark.sql(\"\"\"select EmpID,\n",
    "                            Name,\n",
    "                            Department,\n",
    "                            Salary,\n",
    "                            row_number() over(partition by department order by salary) as row_number_col from \n",
    "                            emp_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+--------------+\n",
      "|EmpID|     Name|Department|Salary|row_number_col|\n",
      "+-----+---------+----------+------+--------------+\n",
      "|  106|   prapti|        A2|    10|             1|\n",
      "|  107|    kamal|        A2|    10|             2|\n",
      "|  108|     Dulo|        A2|    12|             3|\n",
      "|  109|   Kamali|        A2|    12|             4|\n",
      "|  111|   Prpait|        A2|    12|             5|\n",
      "|  110|Prajapati|        A2|    13|             6|\n",
      "|  112|     Mumu|        A3|  1000|             1|\n",
      "|  113|    Shivu|        A3|  1000|             2|\n",
      "|  101|  Arihant|        A1|  1000|             1|\n",
      "|  102| Shashank|        A1|  1000|             2|\n",
      "|  105|    Singh|        A1|  1000|             3|\n",
      "|  104|  Sangram|        A1|  3000|             4|\n",
      "|  103|     Ranu|        A1|   500|             5|\n",
      "+-----+---------+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_number.show(1909)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data frame approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the common snippet windowspec\n",
    "windowSpec = Window.partitionBy(\"Department\").orderBy(\"Salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+--------+\n",
      "|EmpID|     Name|Department|Salary|rank_col|\n",
      "+-----+---------+----------+------+--------+\n",
      "|  106|   prapti|        A2|    10|       1|\n",
      "|  107|    kamal|        A2|    10|       1|\n",
      "|  108|     Dulo|        A2|    12|       3|\n",
      "|  109|   Kamali|        A2|    12|       3|\n",
      "|  111|   Prpait|        A2|    12|       3|\n",
      "|  110|Prajapati|        A2|    13|       6|\n",
      "|  112|     Mumu|        A3|  1000|       1|\n",
      "|  113|    Shivu|        A3|  1000|       1|\n",
      "|  101|  Arihant|        A1|  1000|       1|\n",
      "|  102| Shashank|        A1|  1000|       1|\n",
      "|  105|    Singh|        A1|  1000|       1|\n",
      "|  104|  Sangram|        A1|  3000|       4|\n",
      "|  103|     Ranu|        A1|   500|       5|\n",
      "+-----+---------+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_wala = read_file.withColumn(\"rank_col\", rank().over(windowSpec))\n",
    "rank_wala.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+--------------+\n",
      "|EmpID|     Name|Department|Salary|dense_rank_col|\n",
      "+-----+---------+----------+------+--------------+\n",
      "|  106|   prapti|        A2|    10|             1|\n",
      "|  107|    kamal|        A2|    10|             1|\n",
      "|  108|     Dulo|        A2|    12|             2|\n",
      "|  109|   Kamali|        A2|    12|             2|\n",
      "|  111|   Prpait|        A2|    12|             2|\n",
      "|  110|Prajapati|        A2|    13|             3|\n",
      "|  112|     Mumu|        A3|  1000|             1|\n",
      "|  113|    Shivu|        A3|  1000|             1|\n",
      "|  101|  Arihant|        A1|  1000|             1|\n",
      "|  102| Shashank|        A1|  1000|             1|\n",
      "|  105|    Singh|        A1|  1000|             1|\n",
      "|  104|  Sangram|        A1|  3000|             2|\n",
      "|  103|     Ranu|        A1|   500|             3|\n",
      "+-----+---------+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dense_rank_wala = read_file.withColumn(\"dense_rank_col\", dense_rank().over(windowSpec))\n",
    "dense_rank_wala.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------+-------+\n",
      "|EmpID|     Name|Department|Salary|row_col|\n",
      "+-----+---------+----------+------+-------+\n",
      "|  106|   prapti|        A2|    10|      1|\n",
      "|  107|    kamal|        A2|    10|      2|\n",
      "|  108|     Dulo|        A2|    12|      3|\n",
      "|  109|   Kamali|        A2|    12|      4|\n",
      "|  111|   Prpait|        A2|    12|      5|\n",
      "|  110|Prajapati|        A2|    13|      6|\n",
      "|  112|     Mumu|        A3|  1000|      1|\n",
      "|  113|    Shivu|        A3|  1000|      2|\n",
      "|  101|  Arihant|        A1|  1000|      1|\n",
      "|  102| Shashank|        A1|  1000|      2|\n",
      "|  105|    Singh|        A1|  1000|      3|\n",
      "|  104|  Sangram|        A1|  3000|      4|\n",
      "|  103|     Ranu|        A1|   500|      5|\n",
      "+-----+---------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_number_wala = read_file.withColumn(\"row_col\", row_number().over(windowSpec))\n",
    "row_number_wala.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory :\n",
    "\n",
    "Dense_rank follows the sequence where as rank doesnt follow the sequeence it focuses on positioning after arrnagements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary = spark.sql(\"\"\"select max(salary) as salary from emp_table where \n",
    "salary < (select max(salary) from emp_table) and department=\"A1\" \"\"\")\n",
    "\n",
    "salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------+\n",
      "|EmpID|    Name|Department|salary|\n",
      "+-----+--------+----------+------+\n",
      "|  101| Arihant|        A1|  1000|\n",
      "|  102|Shashank|        A1|  1000|\n",
      "|  103|    Ranu|        A1|   500|\n",
      "|  104| Sangram|        A1|  3000|\n",
      "|  105|   Singh|        A1|  1000|\n",
      "+-----+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmissing ')' at '<EOF>'(line 2, pos 43)\n\n== SQL ==\nselect max(salary) as salary from emp_table where \nsalary < (select max(salary) from emp_table\n-------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-458d114a3609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m salary = spark.sql(\"\"\"select max(salary) as salary from emp_table where \n\u001b[0m\u001b[0;32m      2\u001b[0m salary < (select max(salary) from emp_table\"\"\")\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \"\"\"\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mParseException\u001b[0m: \nmissing ')' at '<EOF>'(line 2, pos 43)\n\n== SQL ==\nselect max(salary) as salary from emp_table where \nsalary < (select max(salary) from emp_table\n-------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "salary = spark.sql(\"\"\"select max(salary) as salary from emp_table where \n",
    "salary < (select max(salary) from emp_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
